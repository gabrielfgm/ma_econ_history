---
title: "Week 2: How to read a regression table in a social science paper" 
author: | 
    | Dr. Gabriel Geisler Mesevage 
    | gabriel.mesevage@kcl.ac.uk
    | Office Hours: Wed. 10-11am & Thurs. 2-3pm
date: "21 January 2020"
output: 
  xaringan::moon_reader:
    nature:
      beforeInit: "macros.js"
    lib_dir: libs
    css: xaringan-themer.css
---

<style type="text/css">
.white { color: white; }
.large { font-size: 130% }
.small { font-size: 70% }
.pull-right40{ 
  float: right;
  width: 40%;}
.pull-left60{
  float: left;
  width: 60%;
}
.scale120{
  float: left;
  width: 120%;
}
.scale80{
  float: right;
  width: 80%
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
solarized_light()
```

## What you will see

![:scale 80%](reg_examp.png)

---

## The equation behind the table

.center[
$\texttt{gdp_growth_pc}_i = \alpha + \beta_1 log(\texttt{initial_income}_i) + \\ \; \beta_2 \texttt{growth_capital_stock}_i + \beta_3 \texttt{growth_land_pc}_i + \\ \; \beta_4 log(\texttt{agric_tariff}_i + 1) + \beta_5 log(\texttt{manuf_tariff}_i + 1) + \\ \; \beta_6 log(\texttt{exotic_tariff}_i + 1) + \epsilon_i$
]

***

+ $\alpha$ and the $\beta$'s are **parameters**: they are estimated from the data
+ The numbers in the table are the **parameters** associated with each **variable**
    - E.g. $\beta_1 = -6.0603$ is the parameter for $\texttt{initial_income}$
+ $\epsilon_i$ is the **error**:
    - The difference between what the model suggests and the data
    - We do not observe the true $\epsilon_i$
    - We *do* observe how the model differs from the data: we call these observations $\hat{\epsilon}$ where the hat denotes an estimate 
    
### Let's start with a simple example

---

## Simple example

```{r, reg_manual, out.width='100%', fig.height=4, fig.retina=4}
library(tidyverse)
library(ggrepel)
library(ggthemes)

set.seed(42)
x <- rnorm(100)
beta <- 1.5
alpha <- 40
eps <- rnorm(100)

y <- alpha + beta*x + eps

fd <- data.frame(y, x, eps)

reg <- lm(y~x, data = fd)

fd %>% 
  ggplot(aes(x, y)) + 
  geom_point(size = 3, color = "tomato") +
  stat_smooth(method = 'lm', se = FALSE) +
  theme_hc() +
  geom_label_repel(data = data.frame(x = 0, y = reg$coefficients[[1]],
                                     lab = "Intercept = 39.9"), 
                   aes(x, y, label = lab), nudge_x = -2, nudge_y = 2) +
  geom_label_repel(data = data.frame(x = -1.5, 
                                     y = predict(reg, 
                                                 newdata = data.frame(x = -1.5)),
                                     lab = paste0("The slope of \nthis line is ", 
                                                  round(reg$coefficients[[2]], 2))), 
                   aes(x, y, label = lab), nudge_x = 3, nudge_y = -2) +
  geom_text(data = data.frame(x = sort(x)[2]-.15, y = 37, lab = "An error"),
                   aes(x, y, label = lab), angle = 90, color = "tomato") + 
  ggtitle("A simple regression") +
  geom_segment(aes(x, y, xend = x, yend = fitted(reg)), linetype="dashed")+
  geom_vline(xintercept = 0) +
  theme(panel.background = element_rect(fill = "#fdf6e3"),
        plot.background = element_rect(fill = "#fdf6e3"))
  

```

#### Interpretation: Linear Conditional Expectation

- The line that makes the best guess at average $y$ given $x$

---

## A simple regression

```{r, reg_output, results='asis'}
stargazer::stargazer(reg, header = FALSE, 
                     title = "Example Regression Table",
                     single.row = TRUE,
                     omit.stat = c("rsq", "f", "ser"), type = "html")
```

+ **Standard errors** are in parantheses
+ A **statistically significant** coefficient means the **slope is non-zero**
    - The stars by $x$ indicate statistical significance
    - The stars by $Constant$ indicate that the intercept is far from zero

---

## Statistical significance

+ We estimated the coefficient on $\beta_1 = 1.527$
+ We estimated its standard error to be $se_1 = .088$
    - This means we think on **average** the coefficient will be $1.527$ with a variance of $se_1^2 = .088^2 = .007744$
+ The mean and average imply a distribution for the coefficient -- is it far from zero?
    - Rule of thumb: multiply the standard-error by 2 and add/subtract from coefficient for 95% confidence interval
    
```{r hist_beta, out.width='100%', fig.width=6, fig.height=2, fig.retina=4}
ggplot(data = data.frame(x = c(-1, 2)), aes(x)) +
  stat_function(fun = dnorm, n = 10000, args = list(mean = 1.527, sd = .088)) + ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_hc() + 
  geom_vline(xintercept = 0) +
  theme(panel.background = element_rect(fill = "#fdf6e3"),
        plot.background = element_rect(fill = "#fdf6e3"))
  
```


---

## Assumptions: the relationship is linear

```{r, reg_manual_nl, out.width='100%', fig.height=4, fig.retina=4}
set.seed(42)
x <- rnorm(100)
beta <- 1.5
alpha <- 40
eps <- rnorm(100)

y <- alpha + beta*x^2 + eps

fd <- data.frame(y, x, eps)

reg2 <- lm(y~x, data = fd)

fd %>% 
  ggplot(aes(x, y)) + 
  geom_point(size = 3, color = "tomato") +
  stat_smooth(method = 'lm', se = FALSE) +
  theme_hc() +
  ggtitle("A bad fit") +
  geom_segment(aes(x, y, xend = x, yend = fitted(reg2)), linetype="dashed")+
  theme(panel.background = element_rect(fill = "#fdf6e3"),
        plot.background = element_rect(fill = "#fdf6e3"))
  

```

+ Here the true model is $y = \alpha + \beta x^2 + \epsilon$ 
    - With two variables plotting your data should usually let you fix a problem like this

---

## What happens with more variables?

```{r, reg_manual_ovb, out.width='100%', fig.height=4, fig.retina=4}
set.seed(4242)
alpha <- 40
eps <- rnorm(1000, mean = 0, sd = 15)
z <- rep(c(0,1), each = 500)
x <- rnorm(1000, mean = z, sd = 3)

y1 <- alpha - x[1:500] + eps[1:500] + 50 * z[1:500]
y2 <- alpha - x[501:1000] + eps[501:1000] + 50 *  z[501:1000]

y <- c(y1, y2)

fd <- data.frame(y, x, z, eps)

reg3 <- lm(y~x, data = fd)

p1 <- fd %>% 
  ggplot(aes(x, y)) + 
  geom_point(size = 3, alpha=.5) +
  stat_smooth(method = 'lm', se = FALSE) +
  theme_hc() +
  #geom_segment(aes(x, y, xend = x, yend = fitted(reg3)), linetype="dashed")+
  theme(panel.background = element_rect(fill = "#fdf6e3"),
        plot.background = element_rect(fill = "#fdf6e3"))
  
p2 <- fd %>% 
  ggplot(aes(x, y, color=as.factor(z), group=as.factor(z))) + 
  geom_point(size = 3, alpha=.5) +
  stat_smooth(method = 'lm', se = FALSE) +
  theme_hc() +
  #ggtitle("Missing variables can change a relationship") +
  #geom_segment(aes(x, y, xend = x, yend = fitted(reg3)), linetype="dashed") +
  scale_color_hc() +
  guides(color = FALSE)+
  theme(panel.background = element_rect(fill = "#fdf6e3"),
        plot.background = element_rect(fill = "#fdf6e3"))

cowplot::plot_grid(p1, p2)

```

---

```{r omv_tabular, results='asis'}
reg3 <- lm(y~x, data = fd)
reg4 <- lm(y~x+z, data = fd)

stargazer::stargazer(reg3, reg4, type = 'html', header = FALSE, 
                     title = "Omitted Variable Bias in Action",
                     single.row = TRUE,
                     omit.stat = c("adj.rsq", "f", "ser"))

```

+ The value a parameter takes depends on the other variables in the model
    - We call this **omitted variable bias**
    - Why you **should not** interpret regressions as **causal relations**
    - Intuition: $z$ is large whenever $x$ is large. If you look only at $y$ and $x$ the effect of $x$ combines the effects of $x$ and $z$
+ Why does randomization produce **causal estimates**?
+ $R^2$: a measure of how much of the variation in $y$ the model explains
---

## Questions to think about when reading a regression

+ Did the authors omit variables that could change the relationship and should be included?
    - The constant concern of any statistical argument
+ Is the model appropriate?
+ Is the relationship of a meaningful magnitude?

---

## Resources

[Morgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference : Methods and Principles for Social Research, Cambridge University Press, 2007](https://ebookcentral.proquest.com/lib/kcl/detail.action?docID=321107): This is an introductory graduate level text frequently used in sociology and political science. It focuses on causal inference from data analysis.

Jeffrey Wooldridge, *Introductory Econometrics: A Modern Approach*, is what is often used with economics undergrads (no digital copy in the library unfortunately).

Joshua D. Angrist & Jorn-Steffen Pischke, *Mostly Harmless Econometrics: An Empiricist's Companion*, is a popular graduate-level treatment for economists that surveys common approaches.